{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"You will seek me and find me when you seek me with all your heart.\" â€”Jeremiah 29:13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This contains notes from pytorch course \n",
    "> https://classroom.udacity.com/courses/ud188/lessons/c5706f76-0e30-4b48-b74e-c19fafc33a75/concepts/4e645270-9102-4683-b1b9-a2376e68f10d![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix multiplication\n",
    "- torch.nn : **preferred method**\n",
    "- torch.matmul : does matrix multiplication but also does some broadcasting that might cause errors to go unnoticed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiply a tensor:\n",
    "- inplace multiplication `_` implies in place --> b_tensor.mul_(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape / Resize\n",
    "- torch.view(n,m)  : **preferred method**\n",
    "    - does not make a copy\n",
    "    - does not change the shape of the original datastructure\n",
    "    - if I dont know how many rows I want but I know how many columns I want then I can just say n=-1. Based on number of columns it will automatically calculate the number of rows\n",
    "    - similarly for unknown # of columns, I can say m=-1. I will have to specify proper `n` value though\n",
    "- torch.reshape(n,m) : sometimes, this can make a copy of the original datastructure\n",
    "- torch.resize(n,m): \n",
    "    - if the dimensions are bigger than the original then you will have random values\n",
    "    - if dimensions are smaller, then some data will be cropped out. \n",
    "    - So it is easy to make errors and not notice with this method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy\n",
    "- to convert tensor to numpy --> torch.numpy(b_tensor)\n",
    "- numpy to tensor --> torch.from_numpy(a_array)\n",
    "memory is shared between np array & torch so any change in 1 is reflected in another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data from Folders\n",
    "- use datasets\n",
    "- datasets are passed as arguments to dataloaders along with batch_size\n",
    "- we can pass dataloaders into iter() to iterate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation function\n",
    "When creating an activation function in a Network we can call it in 2 ways:\n",
    "- nn.sigmoid:\n",
    "    - this will create an object in the network\n",
    "-nn.functional.sigmoid\n",
    "    - this is a preferred and succint way. We dont create an object instead we just use this in forward function\n",
    "    \n",
    "\n",
    "Usually ReLU based networks train faster. In general it is better to avoid using softmax activation as it can give probablity values near zero. Instead, we can use log_softmax. To convert to probability we can call torch.exp() on the results.  The loss function that goes with log_softmax is negative log likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "- In general, for reproducibility it is better to set torch.manual_seed(some_number). This will generate same random values every time\n",
    "- We can set custom set values for weights or biases using:\n",
    "    - model.fc1.bias.data.fill_(0)\n",
    "        - fc1: layer name\n",
    "        - `-` means inplace\n",
    "    - model.fc1.weight.data.normal_(std=0.01)\n",
    "        - this is for custom distribution values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convention for labelling\n",
    "- For fully connected network use 'fc1', 'fc2' etc\n",
    "- Loss is usually assigned to variable with name 'criterion'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "- nn.crossentropy loss\n",
    "    - has both nn.logsoftmax and nn.nllLoss\n",
    "        - so inputs should be raw scores or logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a network\n",
    "- we could create a object called Network, define layers and forward function\n",
    "- nn.sequential is a another way of doing this\n",
    "    - When we use nn.sequential we can access the layers by index. Incase, we want to label the layers, we can pass the labels and layers as ordered dict\n",
    "    - `model= nn.sequential(OrderedDict([\n",
    "    ('fc1',nn.Linear(3,4)),\n",
    "    ('rulu',nn.Relu)]))`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
